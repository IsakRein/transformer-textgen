{
  "model": "transformer",
  "tokenizer": "bpe",
  "batch_size": 16,
  "seq_length": 25,
  "n_iters": 2000,
  "eval_interval": 100,
  "learning_rate": 0.001,
  "eval_iters": 200,
  "n_embd": 64,
  "n_head": 4,
  "n_layer": 4,
  "dropout": 0.1,
  "feed_forward_multiplier": 4,
  "max_new_tokens": 200,
  "d_ff": 256,
  "log_every": 1000,
  "syntesize_every": 10000,
  "train_size": 0.9,
  "lambda": 0.001,
  "sampling": "nucleus",
  "temperature": 0.9,
  "nucleus": 0.9
}
